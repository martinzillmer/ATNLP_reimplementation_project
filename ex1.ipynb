{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "Random seed set as 42\n",
      "Loading data...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Token jump not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_train_simple.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_test_simple.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_df, test_df, voc_in, voc_out \u001b[39m=\u001b[39m get_dataframes(train_url, test_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Create datasets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_data \u001b[39m=\u001b[39m Text_dataset(train_df[[\u001b[39m'\u001b[39m\u001b[39mIN_idx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOUT_idx\u001b[39m\u001b[39m'\u001b[39m]], sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/preprocessing.py:89\u001b[0m, in \u001b[0;36mget_dataframes\u001b[0;34m(train_url, test_url)\u001b[0m\n\u001b[1;32m     86\u001b[0m      y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(voc_out\u001b[39m.\u001b[39mlookup_indices(row\u001b[39m.\u001b[39mIN), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\n\u001b[1;32m     87\u001b[0m      \u001b[39mreturn\u001b[39;00m x, y \n\u001b[0;32m---> 89\u001b[0m train_df[[\u001b[39m'\u001b[39m\u001b[39mIN_idx\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mOUT_idx\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m train_df[[\u001b[39m'\u001b[39;49m\u001b[39mIN\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mOUT\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39;49mapply(text_to_tensor, result_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexpand\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[1;32m     90\u001b[0m test_df[[\u001b[39m'\u001b[39m\u001b[39mIN_idx\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mOUT_idx\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m test_df[[\u001b[39m'\u001b[39m\u001b[39mIN\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mOUT\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mapply(text_to_tensor, result_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexpand\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m train_max_len_out \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(train_df\u001b[39m.\u001b[39mOUT_idx\u001b[39m.\u001b[39mapply(\u001b[39mlen\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py:10037\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10025\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10027\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m  10028\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m  10029\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10035\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m  10036\u001b[0m )\n\u001b[0;32m> 10037\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/apply.py:831\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    829\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 831\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/apply.py:957\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    959\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/apply.py:973\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    971\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    972\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(v, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m    974\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    975\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    976\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    977\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/preprocessing.py:86\u001b[0m, in \u001b[0;36mget_dataframes.<locals>.text_to_tensor\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert tokens into indexes from their respective vocabulary\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(voc_in\u001b[39m.\u001b[39mlookup_indices(row\u001b[39m.\u001b[39mIN), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\n\u001b[0;32m---> 86\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(voc_out\u001b[39m.\u001b[39;49mlookup_indices(row\u001b[39m.\u001b[39;49mIN), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchtext/vocab/vocab.py:142\u001b[0m, in \u001b[0;36mVocab.lookup_indices\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlookup_indices\u001b[39m(\u001b[39mself\u001b[39m, tokens: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m    135\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m        tokens: the tokens used to lookup their corresponding `indices`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m        The 'indices` associated with `tokens`.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookup_indices(tokens)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Token jump not found and default index is not set"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from preprocessing import get_dataframes\n",
    "from dataloading import Text_dataset, get_dataloaders\n",
    "from utilities import set_seed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "# Enforce reproducability\n",
    "set_seed(42)\n",
    "\n",
    "# Create dataframes\n",
    "train_url = \"https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_train_simple.txt\"\n",
    "test_url = \"https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_test_simple.txt\"\n",
    "train_df, test_df, voc_in, voc_out = get_dataframes(train_url, test_url)\n",
    "\n",
    "# Create datasets\n",
    "train_data = Text_dataset(train_df[['IN_idx', 'OUT_idx']], sample=True, size=1000)\n",
    "test_data = Text_dataset(test_df[['IN_idx', 'OUT_idx']], sample=False)\n",
    "\n",
    "# Create\n",
    "train_dataloader, test_dataloader= get_dataloaders(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m encoder \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mEncoderRNN(\u001b[39m'\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(voc_in), hidden_size, layers, dropout)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m decoder \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mDecoderRNN(\u001b[39m'\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m'\u001b[39m, hidden_size, \u001b[39mlen\u001b[39m(voc_out), layers, device, max_len)\u001b[39m.\u001b[39mto(device)      \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/ex1.ipynb#X41sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train(train_dataloader, encoder, decoder, device, save_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mex1_best_\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(i))\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/training.py:65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, encoder, decoder, device, n_epochs, learning_rate, print_every, plot_every, save_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, device)\n\u001b[1;32m     66\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     67\u001b[0m     plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/training.py:36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m decoder_outputs, _, _ \u001b[39m=\u001b[39m decoder(encoder_outputs, encoder_hidden, target_tensor_decoder)\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     33\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m     34\u001b[0m     target_tensor\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     38\u001b[0m \u001b[39m# Clip gradients\u001b[39;00m\n\u001b[1;32m     39\u001b[0m clip_grad_norm_(encoder\u001b[39m.\u001b[39mparameters(), max_gradient_norm)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import models\n",
    "from training import train\n",
    "\n",
    "\n",
    "# Best in experiment 1\n",
    "layers = 2\n",
    "hidden_size = 200\n",
    "dropout = 0\n",
    "max_len = train_df.OUT_idx.apply(len).max()\n",
    "\n",
    "for i in range(5):\n",
    "    set_seed(i)\n",
    "    encoder = models.EncoderRNN('lstm', len(voc_in), hidden_size, layers, dropout).to(device)\n",
    "    decoder = models.DecoderRNN('lstm', hidden_size, len(voc_out), layers, device, max_len).to(device)      \n",
    "    train(train_dataloader, encoder, decoder, device, save_name='ex1_best_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall best\n",
    "layers = 2\n",
    "hidden_size = 200\n",
    "dropout = 0.5\n",
    "max_len = train_df.OUT_idx.apply(len).max()\n",
    "\n",
    "for i in range(5):\n",
    "    set_seed(i)\n",
    "    encoder = models.EncoderRNN('lstm', train_vocab_in.n_words, hidden_size, layers, dropout).to(device)\n",
    "    decoder = models.DecoderRNN('lstm', hidden_size, train_vocab_out.n_words, layers, device, max_len).to(device)\n",
    "    train(train_dataloader, encoder, decoder, device, save_name='ex1_overall_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from training import evaluate\n",
    "# Overall best\n",
    "layers = 2\n",
    "hidden_size = 200\n",
    "dropout = 0.5\n",
    "max_len = train_df.OUT_idx.apply(len).max()\n",
    "\n",
    "acc = 0\n",
    "for i in range(1,6):\n",
    "    encoder = models.EncoderRNN('lstm', train_vocab_in.n_words, hidden_size, layers, dropout).to(device)\n",
    "    decoder = models.DecoderRNN('lstm', hidden_size, train_vocab_out.n_words, layers, device, max_len).to(device)\n",
    "    encoder.load_state_dict(torch.load('models/encoder_ex1_best_'+str(i)+'.pth'))\n",
    "    decoder.load_state_dict(torch.load('models/decoder_ex1_best_'+str(i)+'.pth'))\n",
    "    acc += evaluate(encoder, decoder, test_dataloader, device)\n",
    "\n",
    "print(acc/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "acc = evaluate(encoder, decoder, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best overall\n",
    "max_len = train_df.OUT_idx.apply(len).max()\n",
    "layers = 2\n",
    "hidden_size = 200\n",
    "dropout = 0.5\n",
    "\n",
    "bs = 1 # Batch size\n",
    "\n",
    "percentages = [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64]\n",
    "replications = 5\n",
    "accurcies = torch.zeros(len(percentages), replications)\n",
    "for i, p in enumerate(percentages):\n",
    "    set_seed(42)\n",
    "\n",
    "    idx = int((len(train_df) + len(test_df)) * p)\n",
    "    training_data = Text_dataset(train_df[['IN_idx', 'OUT_idx']][:idx], True, 100000)\n",
    "    \n",
    "    for j in range(replications):\n",
    "            set_seed(i)\n",
    "            encoder = models.EncoderRNN('lstm', train_vocab_in.n_words, hidden_size, layers, dropout).to(device)\n",
    "            decoder = models.DecoderRNN('lstm', hidden_size, train_vocab_out.n_words, layers, device, max_len).to(device)\n",
    "            train_dataloader = DataLoader(training_data, batch_size=bs, shuffle=False)\n",
    "            name = 'ex1_pt2_p'+str(p)[2:]+'_'+str(j)\n",
    "            train(train_dataloader, encoder, decoder, device, save_name=name)\n",
    "            acc = evaluate(encoder, decoder, test_dataloader, device)\n",
    "            accurcies[i,j] = acc\n",
    "\n",
    "txt = str(accurcies)\n",
    "f = open(\"ex1_pt2.txt\", \"a\")\n",
    "f.write(txt)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
