{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IN</th>\n",
       "      <th>OUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[jump, opposite, right, twice, and, turn, oppo...</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_TURN_RIGHT, I_JUMP, I_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[run, opposite, left, after, walk, right, &lt;eos&gt;]</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_WALK, I_TURN_LEFT, I_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[walk, after, run, around, right, twice, &lt;eos&gt;]</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_RUN, I_TURN_RIGHT, I_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[look, around, right, thrice, and, turn, left,...</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_LOOK, I_TURN_RIGHT, I_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[walk, opposite, left, twice, and, walk, oppos...</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_LEFT, I_TURN_LEFT, I_WALK, I_TU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  IN  \\\n",
       "0  [jump, opposite, right, twice, and, turn, oppo...   \n",
       "1   [run, opposite, left, after, walk, right, <eos>]   \n",
       "2    [walk, after, run, around, right, twice, <eos>]   \n",
       "3  [look, around, right, thrice, and, turn, left,...   \n",
       "4  [walk, opposite, left, twice, and, walk, oppos...   \n",
       "\n",
       "                                                 OUT  \n",
       "0  [<sos>, I_TURN_RIGHT, I_TURN_RIGHT, I_JUMP, I_...  \n",
       "1  [<sos>, I_TURN_RIGHT, I_WALK, I_TURN_LEFT, I_T...  \n",
       "2  [<sos>, I_TURN_RIGHT, I_RUN, I_TURN_RIGHT, I_R...  \n",
       "3  [<sos>, I_TURN_RIGHT, I_LOOK, I_TURN_RIGHT, I_...  \n",
       "4  [<sos>, I_TURN_LEFT, I_TURN_LEFT, I_WALK, I_TU...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing import get_dataframes\n",
    "train_url = \"https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_train_simple.txt\"\n",
    "test_url = \"https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_test_simple.txt\"\n",
    "train_df, test_df = get_dataframes(train_url, test_url)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<sos>': 0, '<eos>': 1, 'jump': 2, 'opposite': 3, 'right': 4, 'twice': 5, 'and': 6, 'turn': 7, 'thrice': 8, 'run': 9, 'left': 10, 'after': 11, 'walk': 12, 'around': 13, 'look': 14}\n",
      "{'<sos>': 0, '<eos>': 1, 'I_TURN_RIGHT': 2, 'I_JUMP': 3, 'I_WALK': 4, 'I_TURN_LEFT': 5, 'I_RUN': 6, 'I_LOOK': 7}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IN</th>\n",
       "      <th>OUT</th>\n",
       "      <th>IN_idx</th>\n",
       "      <th>OUT_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[jump, opposite, right, twice, and, turn, oppo...</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_TURN_RIGHT, I_JUMP, I_...</td>\n",
       "      <td>[tensor(2), tensor(3), tensor(4), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(2), tensor(2), tensor(3), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[run, opposite, left, after, walk, right, &lt;eos&gt;]</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_WALK, I_TURN_LEFT, I_T...</td>\n",
       "      <td>[tensor(9), tensor(3), tensor(10), tensor(11),...</td>\n",
       "      <td>[tensor(0), tensor(2), tensor(4), tensor(5), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[walk, after, run, around, right, twice, &lt;eos&gt;]</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_RUN, I_TURN_RIGHT, I_R...</td>\n",
       "      <td>[tensor(12), tensor(11), tensor(9), tensor(13)...</td>\n",
       "      <td>[tensor(0), tensor(2), tensor(6), tensor(2), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[look, around, right, thrice, and, turn, left,...</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_RIGHT, I_LOOK, I_TURN_RIGHT, I_...</td>\n",
       "      <td>[tensor(14), tensor(13), tensor(4), tensor(8),...</td>\n",
       "      <td>[tensor(0), tensor(2), tensor(7), tensor(2), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[walk, opposite, left, twice, and, walk, oppos...</td>\n",
       "      <td>[&lt;sos&gt;, I_TURN_LEFT, I_TURN_LEFT, I_WALK, I_TU...</td>\n",
       "      <td>[tensor(12), tensor(3), tensor(10), tensor(5),...</td>\n",
       "      <td>[tensor(0), tensor(5), tensor(5), tensor(4), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  IN  \\\n",
       "0  [jump, opposite, right, twice, and, turn, oppo...   \n",
       "1   [run, opposite, left, after, walk, right, <eos>]   \n",
       "2    [walk, after, run, around, right, twice, <eos>]   \n",
       "3  [look, around, right, thrice, and, turn, left,...   \n",
       "4  [walk, opposite, left, twice, and, walk, oppos...   \n",
       "\n",
       "                                                 OUT  \\\n",
       "0  [<sos>, I_TURN_RIGHT, I_TURN_RIGHT, I_JUMP, I_...   \n",
       "1  [<sos>, I_TURN_RIGHT, I_WALK, I_TURN_LEFT, I_T...   \n",
       "2  [<sos>, I_TURN_RIGHT, I_RUN, I_TURN_RIGHT, I_R...   \n",
       "3  [<sos>, I_TURN_RIGHT, I_LOOK, I_TURN_RIGHT, I_...   \n",
       "4  [<sos>, I_TURN_LEFT, I_TURN_LEFT, I_WALK, I_TU...   \n",
       "\n",
       "                                              IN_idx  \\\n",
       "0  [tensor(2), tensor(3), tensor(4), tensor(5), t...   \n",
       "1  [tensor(9), tensor(3), tensor(10), tensor(11),...   \n",
       "2  [tensor(12), tensor(11), tensor(9), tensor(13)...   \n",
       "3  [tensor(14), tensor(13), tensor(4), tensor(8),...   \n",
       "4  [tensor(12), tensor(3), tensor(10), tensor(5),...   \n",
       "\n",
       "                                             OUT_idx  \n",
       "0  [tensor(0), tensor(2), tensor(2), tensor(3), t...  \n",
       "1  [tensor(0), tensor(2), tensor(4), tensor(5), t...  \n",
       "2  [tensor(0), tensor(2), tensor(6), tensor(2), t...  \n",
       "3  [tensor(0), tensor(2), tensor(7), tensor(2), t...  \n",
       "4  [tensor(0), tensor(5), tensor(5), tensor(4), t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing import get_vocab\n",
    "\n",
    "train_vocab_in = get_vocab(train_df['IN'], 'in')\n",
    "train_vocab_out = get_vocab(train_df['OUT'], 'out')\n",
    "\n",
    "print(train_vocab_in.word2index)\n",
    "print(train_vocab_out.word2index)\n",
    "\n",
    "def series2idx(row):\n",
    "    x = train_vocab_in.col2idx(row, 'IN')\n",
    "    y = train_vocab_out.col2idx(row, 'OUT')\n",
    "    return x, y\n",
    "    \n",
    "train_df[['IN_idx','OUT_idx']] = train_df.apply(series2idx, axis=1, result_type='expand')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloading import Text_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create datasets\n",
    "training_data = Text_dataset(train_df[['IN_idx', 'OUT_idx']], True, 100000)\n",
    "#test_data = Text_dataset(test_df, False)\n",
    "\n",
    "# Define dataloaders\n",
    "bs = 1 # Batch size\n",
    "train_dataloader = DataLoader(training_data, batch_size=bs, shuffle=False)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  3, 10,  8,  6, 14, 13,  4,  5,  1]])\n",
      "tensor([[0, 5, 5, 5, 5, 5, 5, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 1]])\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    xs, ys = data\n",
    "    print(xs)\n",
    "    print(ys)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/stuff.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/stuff.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m encoder \u001b[39m=\u001b[39m EncoderRNN(train_vocab_in\u001b[39m.\u001b[39mn_words, hidden_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/stuff.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m decoder \u001b[39m=\u001b[39m DecoderRNN(hidden_size, train_vocab_out\u001b[39m.\u001b[39mn_words, max_len, device)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/martin/kode/kurser/ATNLP/ATNLP_reimplementation_project/stuff.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train(train_dataloader, encoder, decoder)\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/training.py:51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, encoder, decoder, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[1;32m     48\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     52\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     53\u001b[0m     plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/training.py:20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m encoder_outputs, encoder_hidden \u001b[39m=\u001b[39m encoder(input_tensor)\n\u001b[0;32m---> 20\u001b[0m decoder_outputs, _, _ \u001b[39m=\u001b[39m decoder(encoder_outputs, encoder_hidden, target_tensor)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     23\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m     24\u001b[0m     target_tensor\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kode/kurser/ATNLP/ATNLP_reimplementation_project/models.py:35\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, encoder_outputs, encoder_hidden, target_tensor)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, encoder_outputs, encoder_hidden, target_tensor\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m     batch_size \u001b[39m=\u001b[39m encoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     decoder_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mempty(batch_size, \u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39mfill_(SOS_token)\n\u001b[1;32m     36\u001b[0m     decoder_hidden \u001b[39m=\u001b[39m encoder_hidden\n\u001b[1;32m     37\u001b[0m     decoder_outputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from models import EncoderRNN, DecoderRNN\n",
    "from training import train\n",
    "max_len = 10\n",
    "hidden_size = 200\n",
    "\n",
    "encoder = EncoderRNN(train_vocab_in.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, train_vocab_out.n_words, max_len, device).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9, 10, 11,  7, 13, 10,  1]])\n",
      "tensor([[[-0.2314,  0.6078,  0.2509, -1.4611,  1.8004, -0.6848, -1.1059,\n",
      "          -0.5177, -0.8370,  0.0650],\n",
      "         [-0.0566,  0.8017,  0.5498, -0.2374, -1.0558, -1.6312,  0.2324,\n",
      "          -0.1150,  1.5447, -0.0980],\n",
      "         [-0.4974,  0.4541,  0.4621, -0.2882,  1.4121, -1.2635,  0.1119,\n",
      "           1.1863,  1.2696,  0.9653],\n",
      "         [-1.2161, -0.7304, -0.8373, -0.2299, -0.6048,  0.6078, -0.9567,\n",
      "           2.1896,  0.6506, -0.3528],\n",
      "         [-1.1912,  0.5624, -1.0473,  0.3324,  0.8719, -1.4345,  0.0536,\n",
      "           0.1425, -1.4736, -0.1098],\n",
      "         [-0.0566,  0.8017,  0.5498, -0.2374, -1.0558, -1.6312,  0.2324,\n",
      "          -0.1150,  1.5447, -0.0980],\n",
      "         [ 0.8730, -0.2601, -0.6206,  0.3851, -1.2088,  0.6031,  1.5122,\n",
      "           1.4980, -0.2072, -0.0791]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 7, 10])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
